This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
notebooks/1_train_policy_model.ipynb
README.md
scripts/explore_raw_data.py
scripts/investigate_data.py
scripts/verify_training_data.py
src/proactive_tutor/data_augmentation.py
src/proactive_tutor/feature_engineering.py
src/proactive_tutor/scheduler.py
src/reasoning_engine/build_rag_index.py
src/reasoning_engine/finetune.py
src/reasoning_engine/prepare_data.py
tests/create_golden_set.py
tests/test_data_quality.py
tests/test_evaluation.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
# Python virtual environment
.venv/
venv/
__pycache__/

# Data and Models (gitignore these directories)
data/
models/

# IDE and OS files
.vscode/
.idea/
.DS_Store

# Reports and logs
*.txt
*.log
</file>

<file path="notebooks/1_train_policy_model.ipynb">
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import re\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyBKT.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Define Paths and Constants ---\n",
    "PROCESSED_DATA_DIR = 'data/processed/'\n",
    "MODELS_DIR = 'models/'\n",
    "\n",
    "# --- THIS IS THE KEY CHANGE: Point directly to the final data file ---\n",
    "MODEL_DATA_FILE = os.path.join(PROCESSED_DATA_DIR, \"LGBM_MODEL_DATA.parquet\")\n",
    "\n",
    "print(\"Setup complete. Ready to load pre-processed data and train final models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(MODEL_DATA_FILE):\n",
    "    df_model_data = pd.read_parquet(MODEL_DATA_FILE)\n",
    "    print(f\"Successfully loaded {len(df_model_data):,} feature-rich interactions.\")\n",
    "    print(\"Columns available:\", df_model_data.columns.tolist())\n",
    "else:\n",
    "    print(f\"FATAL: Final model data file not found at '{MODEL_DATA_FILE}'.\")\n",
    "    print(\"Please ensure you have downloaded the results from Colab and placed the file in the correct directory.\")\n",
    "    df_model_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not df_model_data.empty:\n",
    "    print(\"\\n--- Training ENRICHED LGBM 'Success Predictor' Model ---\")\n",
    "\n",
    "    # We need to get the embedding column names dynamically from the dataframe\n",
    "    embedding_cols = [col for col in df_model_data.columns if col.startswith('embed_')]\n",
    "    \n",
    "    base_features = ['prior_response_time', 'prior_is_correct', 'skill_id_encoded', 'skill_attempts', 'skill_correct_rate', 'question_length', 'bkt_prior_mastery']\n",
    "    features = base_features + embedding_cols\n",
    "    target = 'is_correct'\n",
    "\n",
    "    # Ensure all features are actually in the dataframe before using them\n",
    "    features = [f for f in features if f in df_model_data.columns]\n",
    "\n",
    "    train_df, val_df = train_test_split(df_model_data, test_size=0.2, random_state=42, stratify=df_model_data['student_id'])\n",
    "\n",
    "    X_train, y_train = train_df[features], train_df[target]\n",
    "    X_val, y_val = val_df[features], val_df[target]\n",
    "\n",
    "    print(f\"Training on {len(X_train):,} interactions with {len(features)} features.\")\n",
    "\n",
    "    lgbm_predictor = lgb.LGBMClassifier(objective='binary', metric='auc', random_state=42)\n",
    "    lgbm_predictor.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(10, verbose=False)])\n",
    "\n",
    "    auc = roc_auc_score(y_val, lgbm_predictor.predict_proba(X_val)[:, 1])\n",
    "    print(f\"\\nEnriched LGBM Model AUC on validation set: {auc:.4f}\")\n",
    "\n",
    "    lgbm_model_path = os.path.join(MODELS_DIR, 'lgbm_psych_predictor_enriched.joblib')\n",
    "    joblib.dump(lgbm_predictor, lgbm_model_path)\n",
    "    print(f\"Enriched LGBM 'Tactician' model saved to: {lgbm_model_path}\")\n",
    "\n",
    "    print(\"\\n--- Feature Importance for Enriched Q&A Tactician ---\")\n",
    "    if hasattr(lgbm_predictor, 'feature_importances_'):\n",
    "         lgb.plot_importance(lgbm_predictor, figsize=(10, 8), max_num_features=20, importance_type='gain', title='Top 20 Feature Importances')\n",
    "         plt.tight_layout()\n",
    "         plt.show()\n",
    "else:\n",
    "    print(\"Modeling data not available. Skipping LGBM training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This final cell is for the showcase and is now purely for demonstration.\n",
    "# It is kept separate and will likely be moved to a dedicated 'inference.py' or 'app.py' script.\n",
    "# For now, it remains here to show the full end-to-end logic.\n",
    "# Note: The showcase logic itself is complex and depends on many variables being in the notebook's state.\n",
    "# It's not included in this final version as it requires a full re-run to populate all variables like `df_psych`, `skill_encoder` etc.\n",
    "# The primary goal of this workflow was to successfully train and save the models, which has been achieved.\n",
    "print(\"Models have been trained and saved successfully.\")\n",
    "print(f\"LGBM predictor is at: {lgbm_model_path}\")\n",
    "print(\"BKT and skill encoder models are in the 'models/' directory.\")\n",
    "print(\"The project is now ready for inference or deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="README.md">
---
title: Psychology Tutor Engine
sdk: docker
app_port: 7860
---

# Psychology Tutor Engine - Interactive Workspace

This Space launches a JupyterLab environment on the free `cpu-basic` tier.

From here you can run the data processing pipeline, the main tutor modeling notebook, or manually start the T5 fine-tuning script.
</file>

<file path="scripts/explore_raw_data.py">
# scripts/explore_raw_data.py

import pandas as pd
from pathlib import Path
import zipfile
import io
import ast
from collections import Counter
from contextlib import redirect_stdout

# --- CONFIGURATION ---
RAW_DATA_ROOT = Path("data/1_raw_source_data")
OUTPUT_FILENAME = "raw_data_diagnostics_report.txt"
SAMPLE_ROWS = 2
SAMPLE_LINES = 5
FILES_TO_IGNORE = ['.DS_Store', 'state.json', 'dataset_info.json', '.arrow']
WIDE_FORMAT_THRESHOLD = 5 # If a column prefix appears this many times, flag as wide.

def print_header(header):
    print("\n" + "="*80)
    print(f"=== {header.upper()} ===")
    print("="*80)

# --- DIAGNOSTICS & REPORTING ---

def diagnose_and_report(df: pd.DataFrame, file_path: Path, summary_findings: list):
    """Analyzes a DataFrame for common structural problems and records findings."""
    print("\n  --- DIAGNOSTICS ---")
    
    # 1. Wide Format Detection
    try:
        prefixes = [col.split('(')[0] for col in df.columns]
        prefix_counts = Counter(prefixes)
        most_common = prefix_counts.most_common(1)[0]
        if most_common[1] > WIDE_FORMAT_THRESHOLD:
            message = f"File appears to be in a WIDE format. Prefix '{most_common[0]}' is repeated {most_common[1]} times."
            print(f"  [!] Issue Detected: {message}")
            summary_findings.append({
                'file': file_path.name,
                'issue': 'Complex Structure (Wide Format)',
                'details': message,
                'action': 'Reshape data from wide to long format (melt).'
            })
            return # Stop further column analysis if it's wide format
    except Exception: pass # Ignore errors in this diagnostic

    # 2. Stringified Object Detection
    for col in df.select_dtypes(include=['object']).columns:
        try:
            sample = df[col].dropna().head(3)
            if sample.empty: continue
            
            # Check if sample values look like dicts or lists and can be parsed
            is_complex = all(val.strip().startswith(('{', '[')) for val in sample)
            if is_complex:
                ast.literal_eval(sample.iloc[0]) # Test parsing on one
                message = f"Column '{col}' appears to contain stringified Python objects (dicts/lists)."
                print(f"  [!] Issue Detected: {message}")
                summary_findings.append({
                    'file': file_path.name,
                    'issue': 'Complex Structure (Stringified Object)',
                    'details': message,
                    'action': 'Use ast.literal_eval to parse this column during ingestion.'
                })
        except (SyntaxError, ValueError, TypeError): continue # Not a valid literal
        except Exception: pass # Ignore other errors

    print("  [+] No major structural issues detected.")

def print_summary_report(findings: list):
    """Prints a summary of all detected issues at the end of the report."""
    print("\n" + "="*80)
    print("--- SUMMARY OF FINDINGS ---")
    print("="*80)

    if not findings:
        print("\nNo programmatic issues were detected in the data files.")
        
    # Manually add non-detectable issues from user analysis for completeness
    findings.append({'file': 'boltmonkey.json, gragroo_train.parquet', 'issue': 'Critical - Licensing', 'action': 'Verify data provenance and license before use.'})
    findings.append({'file': 'All Non-Psychology Sets', 'issue': 'Medium - Domain Mismatch', 'action': 'Consider a staged curriculum learning approach.'})

    # Group findings by issue type for a clean report
    grouped_findings = {}
    for f in findings:
        if f['issue'] not in grouped_findings:
            grouped_findings[f['issue']] = []
        grouped_findings[f['issue']].append(f)

    for issue_type, items in grouped_findings.items():
        print(f"\n--- ISSUE: {issue_type} ---")
        for item in items:
            print(f"  - File(s): {item['file']}")
            if 'details' in item:
                print(f"    Details: {item['details']}")
            print(f"    Action:  {item['action']}")

# --- FILE EXPLORATION LOGIC ---

def explore_dataframe(df: pd.DataFrame, file_path: Path, summary_findings: list):
    """Prints info and runs diagnostics for a pandas DataFrame."""
    print(f"  - Shape: {df.shape}")
    print(f"  - Columns: {df.columns.to_list()}")
    diagnose_and_report(df, file_path, summary_findings)
    print("\n  --- SAMPLE DATA ---")
    with pd.option_context('display.max_columns', None, 'display.width', 120, 'display.max_colwidth', 80):
        print(df.head(SAMPLE_ROWS))

def explore_text_file(reader, file_path: Path):
    """Prints sample lines from a plain text file."""
    print(f"  - File Type: Plain Text. First {SAMPLE_LINES} lines:")
    for i, line in enumerate(reader):
        if i >= SAMPLE_LINES: break
        print(f"    {line.strip()}")

def explore_zip_archive(zip_path: Path, summary: list):
    print(f"\nInspecting ZIP archive: {zip_path.resolve()}")
    with zipfile.ZipFile(zip_path, 'r') as z:
        file_list = [f for f in z.namelist() if Path(f).name not in FILES_TO_IGNORE and not f.startswith('__MACOSX')]
        if not file_list: return print("  - Archive empty or contains only ignored metadata.")
        
        print(f"  - Contains {len(file_list)} file(s): {file_list[:5] if len(file_list) > 5 else file_list}")
        first_data_file = next((f for f in file_list if f.endswith(('.json', '.csv', '.txt', '.jsonl'))), None)

        if not first_data_file: return print("  - No sampleable data files found in archive.")
        
        print(f"\n  --- Sampling first data file from zip: '{first_data_file}' ---")
        with z.open(first_data_file) as internal_file:
            explore_single_file(internal_file, Path(first_data_file), summary)

def explore_single_file(file_handle, file_path: Path, summary: list):
    """Processes a single file object, either from disk or from a zip."""
    try:
        # Handle plain text files first
        if file_path.suffix == '.txt':
            reader = io.TextIOWrapper(file_handle, 'utf-8', errors='ignore')
            explore_text_file(reader, file_path)
            return

        # Handle tabular data with smart encoding
        encoding_used = 'utf-8'
        try:
            content = file_handle.read()
            text_content = content.decode('utf-8')
        except UnicodeDecodeError:
            encoding_used = 'latin-1'
            text_content = content.decode('latin-1')
        except AttributeError: # Already a text stream
            text_content = file_handle.read()
        
        print(f"  - Text Encoding: {encoding_used}")
        # Use StringIO to treat the decoded text as a file for pandas
        text_stream = io.StringIO(text_content)
        
        if file_path.suffix == '.parquet':
            df = pd.read_parquet(file_handle) # Parquet needs original handle
        elif file_path.suffix == '.csv':
            df = pd.read_csv(text_stream, on_bad_lines='skip')
        else: # JSON / JSONL
            df = pd.read_json(text_stream, lines=file_path.suffix == '.jsonl')
        
        explore_dataframe(df, file_path, summary)

    except Exception as e:
        print(f"  [ERROR] Could not read or process file '{file_path.name}'. Reason: {e}")

def main():
    """Main function to run the exploration and save the report."""
    script_dir = Path(__file__).parent
    output_file_path = script_dir / OUTPUT_FILENAME
    print(f"Starting data diagnostics. Report will be saved to:\n{output_file_path.resolve()}")
    
    summary_findings = []
    
    with open(output_file_path, 'w', encoding='utf-8') as f:
        with redirect_stdout(f):
            print("PSYCHOLOGY TUTOR ENGINE - RAW DATA DIAGNOSTICS REPORT")
            for data_dir in sorted(RAW_DATA_ROOT.iterdir()):
                if data_dir.is_dir():
                    print_header(data_dir.name)
                    for file_path in sorted(data_dir.glob("**/*")):
                        if file_path.is_file() and file_path.name not in FILES_TO_IGNORE:
                            if file_path.suffix == '.zip':
                                explore_zip_archive(file_path, summary_findings)
                            else:
                                print(f"\nAnalyzing file: {file_path.resolve()}")
                                with open(file_path, 'rb') as disk_file:
                                    explore_single_file(disk_file, file_path, summary_findings)
            
            print_summary_report(summary_findings)

    print("\nDiagnostics complete. Report successfully saved.")

if __name__ == "__main__":
    main()
</file>

<file path="scripts/investigate_data.py">
# investigate_data.py
# A robust script to find and analyze rows in the dataset based on column length.

import pandas as pd
import argparse
import os

# --- Configuration ---
DEFAULT_DATA_FILE = "data/processed/ALL_PSYCHOLOGY_DATA_normalized.parquet"
DEFAULT_COLUMN = "question"
DEFAULT_THRESHOLD = 10

def investigate_column_length(file_path: str, column_name: str, threshold: int, comparison: str):
    """
    Loads a parquet file and prints rows where the length of a specified column
    is less than or greater than a given threshold.
    
    Args:
        file_path (str): The path to the parquet data file.
        column_name (str): The name of the column to investigate.
        threshold (int): The length threshold to check against.
        comparison (str): Either 'less' or 'greater'.
    """
    # --- 1. Input Validation ---
    if not os.path.exists(file_path):
        print(f"Error: Data file not found at '{file_path}'")
        return

    if comparison not in ['less', 'greater']:
        print(f"Error: Invalid comparison type '{comparison}'. Must be 'less' or 'greater'.")
        return

    print(f"--- Starting Investigation ---")
    print(f"File:        {file_path}")
    print(f"Column:      {column_name}")
    print(f"Threshold:   {threshold}")
    print(f"Condition:   Length is {comparison} than {threshold}")
    print("----------------------------\n")

    # --- 2. Data Loading and Analysis ---
    try:
        df = pd.read_parquet(file_path)

        if column_name not in df.columns:
            print(f"Error: Column '{column_name}' not found in the dataset.")
            print(f"Available columns are: {list(df.columns)}")
            return
            
        # Ensure the column is of string type for .str accessor
        df[column_name] = df[column_name].astype(str)
        
        # Apply the filter based on the comparison type
        if comparison == 'less':
            filtered_df = df[df[column_name].str.len() < threshold]
        else: # comparison == 'greater'
            filtered_df = df[df[column_name].str.len() > threshold]

        # --- 3. Reporting Results ---
        num_found = len(filtered_df)
        if num_found > 0:
            print(f"SUCCESS: Found {num_found} rows meeting the condition.\n")
            # Display relevant columns for context
            display_columns = [column_name, 'source', 'answer']
            # Ensure display columns exist before trying to show them
            valid_display_columns = [col for col in display_columns if col in df.columns]
            
            with pd.option_context('display.max_rows', None, 'display.max_colwidth', 100):
                print(filtered_df[valid_display_columns])
        else:
            print("SUCCESS: Found 0 rows meeting the specified condition.")

    except Exception as e:
        print(f"An unexpected error occurred: {e}")


if __name__ == "__main__":
    # --- 4. Command-Line Interface (CLI) Setup ---
    parser = argparse.ArgumentParser(
        description="Investigate a dataset by finding rows with specific column lengths.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter # Shows default values in help message
    )
    
    parser.add_argument(
        "--file",
        default=DEFAULT_DATA_FILE,
        help="Path to the .parquet data file to investigate."
    )
    parser.add_argument(
        "--column",
        default=DEFAULT_COLUMN,
        help="The column whose length you want to check."
    )
    parser.add_argument(
        "--threshold",
        type=int,
        default=DEFAULT_THRESHOLD,
        help="The character length threshold."
    )
    parser.add_argument(
        "--comparison",
        choices=['less', 'greater'],
        default='less',
        help="Set to 'less' to find rows shorter than the threshold, or 'greater' for longer."
    )

    args = parser.parse_args()
    
    investigate_column_length(
        file_path=args.file,
        column_name=args.column,
        threshold=args.threshold,
        comparison=args.comparison
    )
</file>

<file path="scripts/verify_training_data.py">
import pandas as pd
import os
import argparse

# --- Configuration ---
TRAINING_DATA_FILE = "data/training_sets/distractor_generation_training_data.parquet"

def verify_data(file_path: str, num_samples: int):
    """
    Loads the generated training data and prints a random sample
    for human verification.
    """
    print("--- Starting Training Data Verification ---")
    
    # 1. Validate file exists
    if not os.path.exists(file_path):
        print(f"\n❌ FATAL: Training data file not found at '{file_path}'.")
        print("Please run generate_distractor_training_set.py first.")
        return

    print(f"Loading data from '{file_path}'...")
    try:
        df = pd.read_parquet(file_path)
    except Exception as e:
        print(f"\n❌ FATAL: Could not read parquet file. Error: {e}")
        return

    # 2. Take a random sample for review
    if num_samples > len(df):
        print(f"Warning: Requested {num_samples} samples, but dataset only has {len(df)}. Showing all.")
        num_samples = len(df)
        
    sample_df = df.sample(n=num_samples, random_state=42)

    print(f"\nDisplaying {num_samples} random examples for your review:")
    print("-" * 80)

    # 3. Print samples in a readable format
    for i, row in sample_df.iterrows():
        print(f"\n--- Example {i+1}/{num_samples} ---")
        print(f"\n[QUESTION]:")
        print(f"  {row['question']}")
        print(f"\n  [CORRECT ANSWER]:")
        print(f"    {row['correct_answer']}")
        print(f"\n  [GENERATED DISTRACTOR (is this a good distractor?)]:")
        print(f"    {row['distractor']}")
        print("-" * 80)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Spot-check the quality of the auto-generated distractor training data.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "--file",
        default=TRAINING_DATA_FILE,
        help="Path to the training data .parquet file."
    )
    parser.add_argument(
        "-n", "--num_samples",
        type=int,
        default=5,
        help="The number of random samples to display for verification."
    )

    args = parser.parse_args()
    
    verify_data(file_path=args.file, num_samples=args.num_samples)
</file>

<file path="src/proactive_tutor/data_augmentation.py">
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import os
from tqdm import tqdm

# --- Configuration ---
DATA_FILE = "data/processed/psychology_data_with_embeddings.parquet"
OUTPUT_DIR = "data/training_sets"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "distractor_generation_training_data.parquet")

NUM_SAMPLES_TO_GENERATE = 50000 
SIMILARITY_MIN = 0.3
SIMILARITY_MAX = 0.7
# Process in batches to balance speed and memory usage.
# This size is safe for most standard computers.
BATCH_SIZE = 1000 

if __name__ == "__main__":
    print("--- Starting FAST & SAFE Automated Training Set Generation ---")
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    if not os.path.exists(DATA_FILE):
        print(f"FATAL: Data file with embeddings not found at '{DATA_FILE}'. Please run compute_embeddings.py first.")
    else:
        print("Loading data and embeddings...")
        df = pd.read_parquet(DATA_FILE)
        
        if NUM_SAMPLES_TO_GENERATE > len(df):
            NUM_SAMPLES_TO_GENERATE = len(df)

        df_sample = df.sample(n=NUM_SAMPLES_TO_GENERATE, random_state=42)

        embedding_cols = [col for col in df.columns if col.startswith('embed_')]
        all_embeddings = df[embedding_cols].values
        
        training_records = []
        num_batches = int(np.ceil(len(df_sample) / BATCH_SIZE))

        print(f"Processing {len(df_sample)} samples in {num_batches} batches of size {BATCH_SIZE}...")

        for i in tqdm(range(num_batches)):
            # Get the current batch of questions
            batch_start = i * BATCH_SIZE
            batch_end = (i + 1) * BATCH_SIZE
            batch_df = df_sample.iloc[batch_start:batch_end]
            batch_embeddings = batch_df[embedding_cols].values
            
            # --- FAST MATRIX OPERATION ---
            # Calculate similarity for the entire batch at once.
            # This creates a small, temporary matrix (e.g., 1000 x 400k)
            sim_matrix_batch = cosine_similarity(batch_embeddings, all_embeddings)
            
            # Now, iterate through the results for this small batch
            for j in range(len(batch_df)):
                scores = sim_matrix_batch[j]
                
                candidate_indices = np.where((scores > SIMILARITY_MIN) & (scores < SIMILARITY_MAX))[0]
                
                # Get the original index of the current question
                current_question_original_index = batch_df.index[j]
                candidate_indices = candidate_indices[candidate_indices != current_question_original_index]
                
                if len(candidate_indices) > 0:
                    distractor_idx = np.random.choice(candidate_indices)
                else:
                    sorted_indices = np.argsort(scores)
                    fallback_choice = sorted_indices[int(len(sorted_indices) * 0.3)]
                    distractor_idx = fallback_choice if fallback_choice != current_question_original_index else sorted_indices[int(len(sorted_indices) * 0.3) + 1]

                training_records.append({
                    'question': batch_df.iloc[j]['question'],
                    'correct_answer': batch_df.iloc[j]['answer'],
                    'distractor': df.iloc[distractor_idx]['answer']
                })

        print("\nConstructing final training set from processed records...")
        training_data = pd.DataFrame(training_records)
        
        training_data.to_parquet(OUTPUT_FILE, index=False)
        
        print("\n--- SUCCESS ---")
        print(f"Automatically generated a training set with {len(training_data)} examples.")
        print(f"File saved to: '{OUTPUT_FILE}'")
        print("\nFirst 5 examples:")
        print(training_data.head())
</file>

<file path="src/proactive_tutor/feature_engineering.py">
# feature_engineering.py

import pandas as pd
import numpy as np
import joblib
import os
import re
from pyBKT.models import Model

def simulate_student_interactions(df_qa, num_students, interactions_per_student):
    """Generates a realistic, time-series log from static Q&A data."""
    if df_qa is None or df_qa.empty:
        return pd.DataFrame()

    print(f"\nSimulating interaction logs for {num_students} students...")
    all_interactions = []

    for student_id in range(num_students):
        student_interactions = df_qa.sample(n=interactions_per_student, replace=True).copy()
        student_interactions['student_id'] = student_id

        mastery = {source: 0.1 for source in df_qa['source'].unique()}
        correct_list = []
        for _, row in student_interactions.iterrows():
            source = row['source']
            is_correct = 1 if np.random.rand() < mastery.get(source, 0.1) else 0
            correct_list.append(is_correct)
            mastery[source] += (1 - mastery.get(source, 0.1)) * 0.25 if is_correct else -mastery.get(source, 0.1) * 0.1
            mastery[source] = np.clip(mastery[source], 0.01, 0.99)

        student_interactions['is_correct'] = correct_list

        correct_times = np.random.normal(25, 5, size=len(student_interactions))
        incorrect_times = np.random.normal(60, 15, size=len(student_interactions))
        student_interactions['response_time_sec'] = np.where(student_interactions['is_correct'] == 1, correct_times, incorrect_times).clip(5, 300)

        student_interactions['timestamp'] = pd.to_datetime(pd.Timestamp.now() + pd.to_timedelta(np.arange(len(student_interactions)), 'm'))
        all_interactions.append(student_interactions)

    df_simulated = pd.concat(all_interactions, ignore_index=True) if all_interactions else pd.DataFrame()
    print(f"Simulation complete. Generated {len(df_simulated):,} interactions.")
    return df_simulated

# --- Helper function for BKT prior mastery calculation ---
def _calculate_bkt_mastery_for_history(student_skill_history_df, bkt_models, skill_name):
    """
    Calculates the prior mastery probability for a skill based on a student's history
    using the corresponding BKT model.
    """
    bkt_model = bkt_models.get(skill_name)
    if not bkt_model:
        return [0.5] * len(student_skill_history_df)

    bkt_data = student_skill_history_df.copy()
    bkt_data['user_id'] = 0 # Dummy user_id for this slice
    bkt_data['skill_name'] = skill_name
    bkt_data['correct'] = bkt_data['is_correct']
    bkt_data['order_id'] = np.arange(len(bkt_data)) + 1

    try:
        # --- THIS IS THE FIX ---
        # The .predict() method does NOT take a 'skills' argument.
        # It uses the skill_name column from the provided data.
        predictions = bkt_model.predict(data=bkt_data)
        # --- END OF FIX ---

        initial_prior = bkt_model.params().get(skill_name, [[0.1, 0.1, 0.2, 0.5]])[0][3]
        prior_masteries = [initial_prior] + predictions['state_predictions'].iloc[:-1].tolist()
        
        if len(prior_masteries) != len(student_skill_history_df):
             print(f"Warning: Mismatch in BKT prior calculation for skill {skill_name}. Expected {len(student_skill_history_df)}, got {len(prior_masteries)}. Using default prior.")
             return [0.5] * len(student_skill_history_df)

        return prior_masteries
    except Exception as e:
        print(f"Warning: BKT prediction failed for skill {skill_name} on student history: {e}. Using default prior.")
        return [0.5] * len(student_skill_history_df)


def create_features(df, skill_encoder, bkt_models=None):
    """
    Takes a dataframe of student interactions and engineers the features
    needed for the LGBM model.
    """
    processed_df = df.copy()

    known_sources = skill_encoder.classes_
    processed_df = processed_df[processed_df['source'].isin(known_sources)].copy()

    if processed_df.empty:
        return pd.DataFrame()

    processed_df['skill_id_encoded'] = skill_encoder.transform(processed_df['source'])
    processed_df.sort_values(['student_id', 'timestamp'], inplace=True, kind='mergesort')

    processed_df['prior_is_correct'] = processed_df.groupby('student_id')['is_correct'].shift(1)
    processed_df['prior_response_time'] = processed_df.groupby('student_id')['response_time_sec'].shift(1)
    processed_df['skill_attempts'] = processed_df.groupby(['student_id', 'skill_id_encoded']).cumcount()
    processed_df['skill_correct_sum_prev'] = processed_df.groupby(['student_id', 'skill_id_encoded'])['is_correct'].cumsum().shift(1).fillna(0)
    processed_df['skill_correct_rate'] = processed_df['skill_correct_sum_prev'] / processed_df['skill_attempts']
    processed_df['skill_correct_rate'] = processed_df['skill_correct_rate'].fillna(0.5)
    processed_df.loc[processed_df['skill_attempts'] == 0, 'skill_correct_rate'] = 0.5
    processed_df.drop(columns=['skill_correct_sum_prev'], inplace=True)

    processed_df['question_length'] = processed_df['question'].str.len().fillna(0)

    if bkt_models:
        print("Calculating BKT prior mastery for each interaction...")
        all_masteries = []
        # We need to process group by group to handle sequential nature of BKT
        grouped = processed_df.groupby(['student_id', 'skill_id_encoded'])
        for _, group_df in grouped:
            # We must get the original skill name to look up the correct BKT model
            skill_name = skill_encoder.inverse_transform([group_df['skill_id_encoded'].iloc[0]])[0]
            masteries = _calculate_bkt_mastery_for_history(group_df, bkt_models, skill_name)
            all_masteries.extend(masteries)
        
        # This assignment assumes the order of processed_df is preserved
        processed_df['bkt_prior_mastery'] = all_masteries
    else:
        print("BKT models not provided. 'bkt_prior_mastery' column will be filled with default values.")
        processed_df['bkt_prior_mastery'] = 0.5

    processed_df.dropna(subset=['prior_is_correct', 'prior_response_time'], inplace=True)
    
    return processed_df
</file>

<file path="src/proactive_tutor/scheduler.py">
# scheduler.py
import pandas as pd
from datetime import datetime, timedelta
import joblib
import os
import re

class SpacedRepetitionScheduler:
    INTERVALS = [1, 3, 7, 14, 30, 90, 180, 365] 
    BKT_MASTERY_THRESHOLD = 0.90

    def __init__(self, bkt_models_dir="models/", skill_encoder_path="models/psych_skill_encoder.joblib"):
        self.student_skill_states = {}
        self.bkt_models = self._load_bkt_models(bkt_models_dir)
        self.skill_encoder = joblib.load(skill_encoder_path) if os.path.exists(skill_encoder_path) else None
        
        if not self.skill_encoder:
            print("Warning: Skill encoder not found. Scheduler will operate on skill names directly.")

    def _load_bkt_models(self, models_dir):
        bkt_models = {}
        if not os.path.exists(models_dir):
            print(f"Warning: BKT models directory '{models_dir}' not found. Spaced repetition will be less accurate.")
            return bkt_models
        
        for filename in os.listdir(models_dir):
            if filename.startswith('bkt_model_') and filename.endswith('.pkl'):
                # We need to reconstruct the original skill name from the safe filename
                safe_skill_name = filename.replace('bkt_model_', '').replace('.pkl', '')
                
                # This logic assumes the only special character replaced was '/'
                # You may need to make this more robust if other characters were replaced
                skill_name_reconstructed = re.sub(r'_', '/', safe_skill_name)
                
                try:
                    # --- THIS IS THE FIX ---
                    # Use joblib.load() instead of Model.load()
                    bkt_models[skill_name_reconstructed] = joblib.load(os.path.join(models_dir, filename))
                    # --- END OF FIX ---
                except Exception as e:
                    print(f"Error loading BKT model {filename}: {e}")
        return bkt_models

    def update_skill_status(self, student_id: int, skill_name: str, is_correct: int, current_timestamp: datetime, bkt_mastery_after_interaction: float):
        if student_id not in self.student_skill_states:
            self.student_skill_states[student_id] = {}
        
        current_state = self.student_skill_states[student_id].get(skill_name, {'interval_idx': 0, 'last_practice_time': current_timestamp, 'bkt_mastery_at_last_update': 0.0})

        if is_correct == 1 and bkt_mastery_after_interaction >= self.BKT_MASTERY_THRESHOLD:
            current_state['interval_idx'] = min(current_state['interval_idx'] + 1, len(self.INTERVALS) - 1)
        elif is_correct == 0:
            current_state['interval_idx'] = 0

        current_state['last_practice_time'] = current_timestamp
        current_state['bkt_mastery_at_last_update'] = bkt_mastery_after_interaction
        self.student_skill_states[student_id][skill_name] = current_state

    def get_skills_due_for_review(self, student_id: int, current_timestamp: datetime) -> list:
        due_skills = []
        if student_id not in self.student_skill_states:
            return due_skills
        
        for skill_name, state in self.student_skill_states[student_id].items():
            last_practice_time = state['last_practice_time']
            interval_idx = state['interval_idx']
            
            if interval_idx == 0 and state['bkt_mastery_at_last_update'] < self.BKT_MASTERY_THRESHOLD:
                continue
            
            next_interval_days = self.INTERVALS[min(interval_idx, len(self.INTERVALS) - 1)]
            next_review_time = last_practice_time + timedelta(days=next_interval_days)
            
            if current_timestamp >= next_review_time:
                due_skills.append(skill_name)
        
        return due_skills
</file>

<file path="src/reasoning_engine/finetune.py">
import os
from datasets import load_dataset
from transformers import (
    T5ForConditionalGeneration,
    T5Tokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)

# --- Configuration ---------------------------------------------------------

TRAINING_DATA_REPO = "adfras/psychology-distractor-data"
BASE_MODEL          = "t5-small"
OUTPUT_MODEL_DIR    = "/data/distractor_generator_t5_small"   # survives restarts

os.makedirs(OUTPUT_MODEL_DIR, exist_ok=True)

# --- Main ------------------------------------------------------------------

if __name__ == "__main__":
    print(f"Fine-tuning {BASE_MODEL} on {TRAINING_DATA_REPO}")

    # 1  Load and subsample the dataset directly on the Hub object
    hf_dataset = (
        load_dataset(TRAINING_DATA_REPO, split="train")
        .shuffle(seed=42)
        .select(range(min(10_000, len(load_dataset(TRAINING_DATA_REPO, split='train')))))
    )
    print(f"Loaded {len(hf_dataset)} examples")

    # 2  Tokenizer and model
    tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL)
    model     = T5ForConditionalGeneration.from_pretrained(BASE_MODEL)

    # 3  Pre-processing
    def preprocess(examples):
        prefix   = "generate distractor: "
        inputs   = [
            f"{prefix}question: {q} answer: {a}"
            for q, a in zip(examples["question"], examples["correct_answer"])
        ]
        model_in = tokenizer(
            inputs,
            max_length=512,
            truncation=True,
            padding="max_length",
        )
        labels = tokenizer(
            text_target=examples["distractor"],
            max_length=128,
            truncation=True,
            padding="max_length",
        )
        model_in["labels"] = labels["input_ids"]
        return model_in

    tokenized_dataset = hf_dataset.map(
        preprocess,
        batched=True,
        remove_columns=hf_dataset.column_names,
    )

    # 4  Data collator (avoids double padding)
    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="pt")

    # 5  Training arguments
    training_args = TrainingArguments(
        output_dir=OUTPUT_MODEL_DIR,
        num_train_epochs=3,
        per_device_train_batch_size=8,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_strategy="steps",
        logging_steps=100,
        save_strategy="epoch",
        save_total_limit=2,
    )

    # 6  Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )

    print("Starting fine-tuning…")
    trainer.train()

    # 7  Save final artefacts
    trainer.save_model(OUTPUT_MODEL_DIR)
    tokenizer.save_pretrained(OUTPUT_MODEL_DIR)
    print(f"Model saved to {OUTPUT_MODEL_DIR}")
</file>

<file path="tests/create_golden_set.py">
# create_golden_set.py
# This script should be run only ONCE to create the permanent benchmark dataset.

import pandas as pd
import os

# --- Configuration ---
PROCESSED_DATA_FILE = "data/processed/ALL_PSYCHOLOGY_DATA_normalized.parquet"
TESTS_DIR = "tests"
GOLDEN_SET_PATH = os.path.join(TESTS_DIR, "golden_test_set.parquet")
SAMPLE_SIZE = 1000

# --- Main Logic ---
if __name__ == "__main__":
    print("--- Creating Golden Test Set ---")

    # 1. Ensure the tests directory exists
    os.makedirs(TESTS_DIR, exist_ok=True)
    
    # 2. Check if the source data file exists
    if not os.path.exists(PROCESSED_DATA_FILE):
        print(f"FATAL: Source data file not found at '{PROCESSED_DATA_FILE}'.")
        print("Please run normalize_psych_data.py first.")
    
    # 3. Check if the golden set already exists
    elif os.path.exists(GOLDEN_SET_PATH):
        print(f"INFO: Golden test set already exists at '{GOLDEN_SET_PATH}'. No action taken.")
        print("If you need to recreate it, please delete the old file first.")
    
    # 4. Create the file if it's missing
    else:
        try:
            print(f"Loading source data from '{PROCESSED_DATA_FILE}'...")
            df = pd.read_parquet(PROCESSED_DATA_FILE)
            
            print(f"Taking a fixed, random sample of {SAMPLE_SIZE} rows...")
            # Using random_state=42 ensures the sample is the same every time
            golden_set = df.sample(n=SAMPLE_SIZE, random_state=42)
            
            print(f"Saving golden test set to '{GOLDEN_SET_PATH}'...")
            golden_set.to_parquet(GOLDEN_SET_PATH, index=False)
            
            print("\nSUCCESS: Golden test set created successfully.")
            
        except Exception as e:
            print(f"\nAn error occurred: {e}")
</file>

<file path="tests/test_data_quality.py">
# tests/test_data_quality.py
import pytest
import pandas as pd
from pathlib import Path

# --- Configuration Synchronized with prepare_data.py ---
PROCESSED_DATA_FILE = Path("data/2_processed_data/normalized_questions.parquet")

# The columns we ACTUALLY generate.
# Note: 'licence' is not part of the current pipeline, so it is not tested for.
EXPECTED_COLUMNS = ['question', 'answer', 'source']

# The data sources we ACTUALLY process. This ensures no unexpected data has been added.
EXPECTED_SOURCES = [
    'gsm8k',
    'pubmedqa',
    'boltmonkey',
    'mentat',
    'gragroo',
    'proofwriter',
    'thoughtsource',
]

MIN_QUESTION_LENGTH = 10
MAX_QUESTION_LENGTH = 1500 # A great check to prevent overly long content.

@pytest.fixture(scope="module")
def data():
    """A pytest fixture to load the main dataset once for all tests."""
    if not PROCESSED_DATA_FILE.exists():
        pytest.fail(f"FATAL: Processed data file not found at {PROCESSED_DATA_FILE}.")
    return pd.read_parquet(PROCESSED_DATA_FILE)

# --- Test Cases ---

def test_file_exists():
    """Test 1: Ensures the processed data file was actually created."""
    assert PROCESSED_DATA_FILE.exists(), "The final processed parquet file is missing."

def test_schema_is_correct(data):
    """Test 2: Validates that all expected columns are present."""
    actual_columns = set(data.columns)
    for col in EXPECTED_COLUMNS:
        assert col in actual_columns, f"Missing expected column: '{col}'"

def test_no_missing_critical_data(data):
    """Test 3: Ensures there are no nulls in the core 'question' and 'answer' fields."""
    assert data['question'].isnull().sum() == 0, "There are missing values in the 'question' column."
    assert data['answer'].isnull().sum() == 0, "There are missing values in the 'answer' column."

def test_content_plausibility(data):
    """Test 4: Checks if the data content is reasonable (e.g., not too short or long)."""
    shortest_question = data['question'].str.len().min()
    assert shortest_question >= MIN_QUESTION_LENGTH, f"Found a question with length {shortest_question}, which is shorter than the minimum threshold of {MIN_QUESTION_LENGTH}."
    
    longest_question = data['question'].str.len().max()
    assert longest_question <= MAX_QUESTION_LENGTH, f"Found a question with length {longest_question}, which is longer than the maximum threshold of {MAX_QUESTION_LENGTH}."

def test_source_column_is_valid(data):
    """
    Test 5: Checks if the 'source' column contains only known, expected values.
    This is crucial for ensuring data provenance is tracked correctly.
    """
    actual_sources = set(data['source'].unique())
    
    # Find any sources that are in our data but NOT in our official expected list.
    unexpected_sources = actual_sources - set(EXPECTED_SOURCES)
    
    assert not unexpected_sources, f"Found unexpected data sources: {unexpected_sources}. Please update the EXPECTED_SOURCES list in the test if this is intentional."
</file>

<file path="tests/test_evaluation.py">
# test_model_performance.py
import pytest
import pandas as pd
import joblib
import os
from sklearn.metrics import roc_auc_score
from feature_engineering import create_features, simulate_student_interactions

# --- Configuration ---
MODELS_DIR = "models/"
LGBM_MODEL_PATH = os.path.join(MODELS_DIR, "lgbm_psych_predictor_enriched.joblib")
SKILL_ENCODER_PATH = os.path.join(MODELS_DIR, "psych_skill_encoder.joblib")
GOLDEN_TEST_SET_PATH = "tests/golden_test_set.parquet"

MINIMUM_ACCEPTABLE_AUC = 0.75

@pytest.fixture(scope="module")
def models_and_data():
    """A single fixture to set up everything needed for the tests."""
    required_files = [LGBM_MODEL_PATH, SKILL_ENCODER_PATH, GOLDEN_TEST_SET_PATH]
    if not all(os.path.exists(p) for p in required_files):
        pytest.fail("FATAL: One or more required files are missing. Please run the notebook and create the golden test set.")
    
    lgbm_model = joblib.load(LGBM_MODEL_PATH)
    skill_encoder = joblib.load(SKILL_ENCODER_PATH)
    golden_df_static = pd.read_parquet(GOLDEN_TEST_SET_PATH)

    # Use the REAL simulation logic to create a test history
    golden_df_simulated = simulate_student_interactions(
        df_qa=golden_df_static, 
        num_students=50, 
        interactions_per_student=20
    )
    
    # Process the golden data using the EXACT same feature engineering function
    processed_golden_data = create_features(golden_df_simulated, skill_encoder)

    return {
        "lgbm": lgbm_model,
        "processed_data": processed_golden_data
    }

def test_lgbm_performance_threshold(models_and_data):
    """TRUST CHECK 1: Is the model's performance on a fixed dataset above our quality bar?"""
    model = models_and_data["lgbm"]
    test_df = models_and_data["processed_data"]
    
    if test_df.empty:
        pytest.fail("Processed golden test set is empty. Check feature engineering and data.")

    features = ['prior_response_time', 'prior_is_correct', 'skill_id_encoded', 'skill_attempts', 'skill_correct_rate', 'question_length']
    target = 'is_correct'
    
    X_test = test_df[features]
    y_test = test_df[target]
    
    predictions = model.predict_proba(X_test)[:, 1]
    auc_score = roc_auc_score(y_test, predictions)
    
    print(f"\nModel AUC on golden test set: {auc_score:.4f}")
    assert auc_score >= MINIMUM_ACCEPTABLE_AUC, f"Model performance has dropped below the threshold! AUC: {auc_score:.4f} < {MINIMUM_ACCEPTABLE_AUC}"

def test_model_feature_importance_is_stable(models_and_data):
    """TRUST CHECK 2: Does the model still 'think' the same way?"""
    model = models_and_data["lgbm"]
    
    feature_names = model.feature_name_
    importances = model.feature_importances_
    
    importance_dict = dict(zip(feature_names, importances))
    most_important_feature = max(importance_dict, key=importance_dict.get)
    
    print(f"\nMost important feature found: '{most_important_feature}'")
    assert most_important_feature == 'skill_correct_rate', "The model's most important feature has changed unexpectedly from 'skill_correct_rate'."
</file>

<file path="src/reasoning_engine/prepare_data.py">
# src/reasoning_engine/prepare_data.py

import pandas as pd
import numpy as np
from pathlib import Path
import json
import ast
from tqdm import tqdm
import zipfile
import io
import re

tqdm.pandas()

# Define paths
RAW_DATA_ROOT = Path("data/1_raw_source_data")
PROCESSED_DATA_DIR = Path("data/2_processed_data")
OUTPUT_FILE = PROCESSED_DATA_DIR / "normalized_questions.parquet"

# --- ROBUST DATA PROCESSING FUNCTIONS ---

def process_gsm8k(path: Path) -> pd.DataFrame:
    dfs = []
    for f in path.glob("*.parquet"):
        df = pd.read_parquet(f)
        df['answer'] = df['answer'].str.split('#### ').str[-1].str.strip()
        df['source'] = 'gsm8k'; dfs.append(df[['question', 'answer', 'source']])
    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

def process_proofwriter(path: Path) -> pd.DataFrame:
    records = []
    for f in path.glob("*.jsonl"):
        with open(f, 'r', encoding='utf-8') as file:
            for line in file:
                try:
                    data = json.loads(line)
                    en_string = data.get('translation', {}).get('en', '')
                    ro_string = data.get('translation', {}).get('ro', '')
                    q_match = re.search(r'\$question\$\s*=\s*(.*?)\s*;', en_string)
                    c_match = re.search(r'\$context\$\s*=\s*(.*)', en_string)
                    a_match = re.search(r'\$answer\$\s*=\s*(\w+)', ro_string)
                    if q_match and c_match and a_match:
                        full_question = f"[CONTEXT]\n{c_match.group(1).strip()}\n\n[QUESTION]\nIs the following statement true or false?\n'{q_match.group(1).strip()}'"
                        records.append({'question': full_question, 'answer': a_match.group(1).strip(), 'source': 'proofwriter'})
                except (json.JSONDecodeError, AttributeError): continue
    return pd.DataFrame(records)

def process_thoughtsource(zip_path: Path) -> pd.DataFrame:
    if not zip_path.exists(): return pd.DataFrame()
    records = []
    with zipfile.ZipFile(zip_path, 'r') as z:
        json_filename = z.namelist()[0]
        with z.open(json_filename) as f:
            data = json.load(f)
            for split_data in data.values():
                if isinstance(split_data, list): continue
                for dataset_name, dataset_list in split_data.items():
                    if isinstance(dataset_list, list):
                        for item in dataset_list:
                            if isinstance(item, dict) and 'question' in item and 'answer' in item:
                                records.append({'question': item['question'], 'answer': item['answer'], 'source': dataset_name})
    return pd.DataFrame(records)

def process_bioasq(zip_path: Path) -> pd.DataFrame:
    if not zip_path.exists(): return pd.DataFrame()
    records = []
    with zipfile.ZipFile(zip_path, 'r') as z:
        for file_info in z.infolist():
            if file_info.filename.endswith('.json') and not file_info.is_dir():
                with z.open(file_info) as f:
                    data = json.load(f)
                    for q_data in data.get('questions', []):
                        question, snippets = q_data.get('body'), q_data.get('snippets', [])
                        if question and snippets:
                            answer = " ".join([s.get('text', '') for s in snippets])
                            if answer: records.append({'question': question, 'answer': answer, 'source': 'bioasq'})
    return pd.DataFrame(records)

def process_pubmedqa(path: Path) -> pd.DataFrame:
    dfs = []
    for f in path.glob("*.csv"):
        with open(f, 'rb') as fh: content = fh.read().decode('utf-8', errors='ignore')
        df = pd.read_csv(io.StringIO(content));
        if 'question' in df.columns and 'long_answer' in df.columns:
            df = df.rename(columns={'long_answer': 'answer'})
            df['source'] = 'pubmedqa'; dfs.append(df[['question', 'answer', 'source']])
    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

def process_mentat_csv(file_path: Path) -> pd.DataFrame:
    """ Correctly processes the MENTAT data from the reliable CSV file. """
    if not file_path.exists(): return pd.DataFrame()
    with open(file_path, 'rb') as fh: content = fh.read().decode('utf-8', errors='ignore')
    df = pd.read_csv(io.StringIO(content))
    answer_cols = ['answer_a', 'answer_b', 'answer_c', 'answer_d', 'answer_e']
    def find_correct_answer(row):
        try:
            truth_list = ast.literal_eval(row['creator_truth'])
            return row[answer_cols[truth_list.index(1.0)]]
        except: return None
    df['answer'] = df.apply(find_correct_answer, axis=1)
    df = df.rename(columns={'text_male': 'question'})
    df['source'] = 'mentat'; return df[['question', 'answer', 'source']].dropna()

def process_psychology_qa(path: Path) -> pd.DataFrame:
    dfs = []
    boltmonkey_file = path / "boltmonkey.json"
    if boltmonkey_file.exists():
        df_bolt = pd.read_json(boltmonkey_file); df_bolt['source'] = 'boltmonkey'; dfs.append(df_bolt)
    gragroo_file = path / "gragroo_train.parquet"
    if gragroo_file.exists():
        df_grag = pd.read_parquet(gragroo_file)
        def extract_qa(c): return (next((m['value'] for m in c if m['from'] == 'human'),None), next((m['value'] for m in c if m['from'] == 'gpt'),None))
        pairs = df_grag['conversations'].apply(extract_qa)
        df_proc = pd.DataFrame(pairs.tolist(), columns=['question', 'answer']).dropna()
        if not df_proc.empty: df_proc['source'] = 'gragroo'; dfs.append(df_proc)
    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

# --- Main Data Preparation Orchestrator ---

def prepare_all_data():
    print("--- Starting Data Ingestion for Question Bank ---")
    sources_to_process = {
        "GSM8K": (process_gsm8k, RAW_DATA_ROOT / "1_chain_of_thought/GSM8K"),
        "ProofWriter": (process_proofwriter, RAW_DATA_ROOT / "1_chain_of_thought/ProofWriter"),
        "ThoughtSource": (process_thoughtsource, RAW_DATA_ROOT / "1_chain_of_thought/ThoughtSource/ThoughtSource-open-data-snapshot.zip"),
        "BioASQ": (process_bioasq, RAW_DATA_ROOT / "2_scholarly_qa/BioASQ/BIOASQ_JSON.zip"),
        "PubMedQA": (process_pubmedqa, RAW_DATA_ROOT / "2_scholarly_qa/PubMedQA"),
        "MENTAT_CSV": (process_mentat_csv, RAW_DATA_ROOT / "2_scholarly_qa/MENTAT/final_dataset_raw_questions.csv"),
        "PsychologyQA": (process_psychology_qa, RAW_DATA_ROOT / "3_psychology_qa"),
    }
    # ... (rest of the script is unchanged and will work correctly)
    all_dfs = []
    print("\n--- Processing Individual Data Sources ---")
    for name, (func, path) in sources_to_process.items():
        print(f"-> Processing: {name}...")
        try:
            df = func(path)
            if not df.empty and {'question', 'answer', 'source'}.issubset(df.columns):
                all_dfs.append(df); print(f"  [SUCCESS] Found {len(df)} records.")
            else: print(f"  [WARNING] No valid records were extracted.")
        except Exception as e: print(f"  [FATAL ERROR] An exception occurred: {e.__class__.__name__}: {e}")

    print("\nNOTE: Student interaction logs (ASSISTments, KDD Cup) are intentionally excluded from this question bank.")
    if not all_dfs: print("\nError: No data was successfully processed. Halting."); return
    print("\n--- Combining All Processed Sources ---")
    final_df = pd.concat(all_dfs, ignore_index=True).dropna(subset=['question', 'answer'])
    print(f"Total records combined: {len(final_df)}")
    print("\nPerforming final cleaning...")
    final_df['question'] = final_df['question'].astype(str); final_df['answer'] = final_df['answer'].astype(str)
    final_df = final_df[final_df['question'].str.len() > 10].reset_index(drop=True)
    print(f"\nSaving normalized question bank to {OUTPUT_FILE}...")
    PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)
    final_df.to_parquet(OUTPUT_FILE)
    print("\n--- Data Preparation Summary ---")
    print(f"Successfully saved {len(final_df)} records to '{OUTPUT_FILE}'.")
    print("Final source distribution:"); print(final_df['source'].value_counts())
    print("\nQuestion Bank generation complete.")

if __name__ == "__main__":
    prepare_all_data()
</file>

<file path="src/reasoning_engine/build_rag_index.py">
# src/reasoning_engine/build_rag_index.py

import pandas as pd
from sentence_transformers import SentenceTransformer
from pathlib import Path
import torch

# --- Configuration ---
# Correctly pointing to the output of our data quality step
DATA_FILE = Path("data/2_processed_data/normalized_questions.parquet")
OUTPUT_FILE = Path("data/2_processed_data/questions_with_embeddings.parquet")
MODEL_NAME = 'all-MiniLM-L6-v2' 

def compute_embeddings():
    print("--- Starting Embedding Computation ---")

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")
    if device == 'cpu':
        print("WARNING: GPU not found. See options below for accelerating this process.")

    if not DATA_FILE.exists():
        print(f"FATAL: Data file not found at {DATA_FILE}. Please run prepare_data.py first.")
        return

    df = pd.read_parquet(DATA_FILE)

    print(f"Loading sentence-transformer model: '{MODEL_NAME}'...")
    model = SentenceTransformer(MODEL_NAME, device=device)

    print(f"Computing embeddings for {len(df)} questions... (This may take a while)")
    embeddings = model.encode(df['question'].tolist(), show_progress_bar=True)

    embedding_df = pd.DataFrame(embeddings, index=df.index)
    embedding_df = embedding_df.add_prefix('embed_')
    df_with_embeddings = pd.concat([df, embedding_df], axis=1)

    print(f"Saving new dataframe with embeddings to '{OUTPUT_FILE}'...")
    df_with_embeddings.to_parquet(OUTPUT_FILE)

    print("\nSUCCESS: Embeddings computed and saved.")
    print(f"New dataframe shape: {df_with_embeddings.shape}")
    print(f"Output file: {OUTPUT_FILE}")

if __name__ == "__main__":
    compute_embeddings()
</file>

</files>
